{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 零.写在最前\n",
    "\n",
    "该项目源于2022 CCF BDCI 大赛之《[Web攻击检测与分类识别](https://www.datafountain.cn/competitions/596)》\n",
    "\n",
    "• 赛题任务\n",
    "\n",
    "参赛团队需要对前期提供的训练集进行分析，通过特征工程、机器学习和深度学习等方法构建AI模型，实现对每一条样本正确且快速分类，不断提高模型精确率和召回率。待模型优化稳定后，通过无标签测试集评估各参赛团队模型分类效果，以正确率评估各参赛团队模型质量。\n",
    "\n",
    "\n",
    "• 解决思路\n",
    "\n",
    "先通过对训练集关键字提取，再经过PaddleNLP文本分类，快速生成比赛结果文件\n",
    "\n",
    "**快速命令行模式 数据预处理+训练+生成比赛结果：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# 1. 切换到工作目录 + 安装 paddlenlp\n",
    "%cd work/\n",
    "!pip install  paddlenlp --upgrade --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2. 一键生成符合格式的训练集，验证集，测试集\n",
    "!python pre.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 3. 训练\n",
    "!python train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 4. 生成比赛结果\n",
    "!python predict.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一.数据集生成\n",
    "\n",
    "对数据集中user_agent,url,refer,body等字段进行关键字提取。并且生成对应的训练接，验证集，测试集\n",
    "\n",
    "这里用到jieba分词\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/b705656d80ff4fd88582f9717748cf9c10110fb848ee44fc869507d8b4c7bf23)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# pre.py\n",
    "import paddle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from config import Config\n",
    "from paddlenlp.datasets import load_dataset\n",
    "import json\n",
    "import jieba.analyse\n",
    "import random\n",
    "\n",
    "\n",
    "class Pre(object):\n",
    "    labels = {}\n",
    "    labels_ = {}\n",
    "    def __init__(self):\n",
    "        self.cf = Config()\n",
    "        self.dataPath = self.cf.dataPath\n",
    "        self.trainRatio = self.cf.trainRatio\n",
    "\n",
    "    def train(self):\n",
    "        lists = []\n",
    "        for i in range(6):\n",
    "            df = pd.read_csv(self.dataPath+'/train/'+str(i)+'.csv').astype(str)\n",
    "            print(len(df))\n",
    "            for j in range(len(df)):\n",
    "                item = []\n",
    "                item.append(df.loc[j, 'method'])\n",
    "                r0 = jieba.analyse.extract_tags(df.loc[j, 'user_agent'], topK=10)\n",
    "                item.extend(r0)\n",
    "                r1 = jieba.analyse.extract_tags(df.loc[j, 'url'], topK=20)\n",
    "                item.extend(r1)\n",
    "                r2 = jieba.analyse.extract_tags(df.loc[j, 'refer'], topK=10)\n",
    "                item.extend(r2)\n",
    "                r3 = jieba.analyse.extract_tags(df.loc[j, 'body'], topK=20)\n",
    "                item.extend(r3)\n",
    "                item_ = {\n",
    "                    'text': \" \".join(item),\n",
    "                    'label': df.loc[j, 'lable']\n",
    "                }\n",
    "                print(j,item_)\n",
    "                lists.append(item_)\n",
    "        random.shuffle(lists)\n",
    "        print(len(lists))\n",
    "        offset = int(len(lists)*float(self.trainRatio))\n",
    "        trains = lists[0:offset]\n",
    "        valids = lists[offset:]\n",
    "\n",
    "        # 生成新的数据集\n",
    "        trainsF = open(self.cf.minePath+'/train.json', 'w')\n",
    "        validsF = open(self.cf.minePath+'/valid.json', 'w')\n",
    "        \n",
    "        for item in trains:\n",
    "            trainsF.write(json.dumps(item, ensure_ascii=False))\n",
    "            trainsF.write('\\n')\n",
    "\n",
    "        for item in valids:\n",
    "            validsF.write(json.dumps(item, ensure_ascii=False))\n",
    "            validsF.write('\\n')\n",
    "\n",
    "    def test(self):\n",
    "        tests = []\n",
    "        df = pd.read_csv(self.dataPath+'/test/test.csv').astype(str)\n",
    "        print(len(df))\n",
    "        for j in range(len(df)):\n",
    "            item = []\n",
    "            item.append(df.loc[j, 'method'])\n",
    "            r0 = jieba.analyse.extract_tags(\n",
    "                df.loc[j, 'user_agent'], topK=10)\n",
    "            item.extend(r0)\n",
    "            r1 = jieba.analyse.extract_tags(df.loc[j, 'url'], topK=20)\n",
    "            item.extend(r1)\n",
    "            r2 = jieba.analyse.extract_tags(df.loc[j, 'refer'], topK=10)\n",
    "            item.extend(r2)\n",
    "            r3 = jieba.analyse.extract_tags(df.loc[j, 'body'], topK=20)\n",
    "            item.extend(r3)\n",
    "            item_ = {\n",
    "                'id': df.loc[j, 'id'],\n",
    "                'text': \" \".join(item)\n",
    "            }\n",
    "            print(j, item_)\n",
    "            tests.append(item_)\n",
    "        print(len(tests))\n",
    "\n",
    "        # 生成新的数据集\n",
    "        testsF = open(self.cf.minePath+'/test.json', 'w')\n",
    "\n",
    "        for item in tests:\n",
    "            testsF.write(json.dumps(item, ensure_ascii=False))\n",
    "            testsF.write('\\n')\n",
    "pre = Pre()\n",
    "pre.train()\n",
    "pre.test()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 二.数据集预处理\n",
    "\n",
    "将数据集转换成MapDataset格式\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#dataset.py\n",
    "import paddle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from config import Config\n",
    "from paddlenlp.datasets import load_dataset\n",
    "import json\n",
    "\n",
    "\n",
    "def datas(dataPath, mode='train'):\n",
    "    f = open(dataPath+'/'+ mode+'.json')\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        data = line.strip().split('\\t')\n",
    "        data = json.loads(data[0])\n",
    "        text = data['text']\n",
    "        if mode == 'test':\n",
    "            yield {\"id\":data['id'], \"text\": text, \"labels\": [0]}\n",
    "        else:\n",
    "            yield {\"text\": text, \"labels\": [int(data['label'])]}\n",
    "    f.close()\n",
    "\n",
    "\n",
    "class Dataset(object):\n",
    "    labels = {}\n",
    "    labels_ = {}\n",
    "    def __init__(self):\n",
    "        self.cf = Config()\n",
    "        self.dataPath = self.cf.minePath\n",
    "\n",
    "    def getLoader(self, mode = 'train'):\n",
    "        ds = load_dataset(datas, dataPath=self.dataPath,mode=mode,lazy=False)\n",
    "        return ds\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 三.训练模型\n",
    "\n",
    "**模型采用ernie-2.0-base-en**\n",
    "\n",
    "对文本进行分词操作\n",
    "\n",
    "设置相关配置：\n",
    "batch_size = 30（32G显卡）\n",
    "epochs = 100\n",
    "learning_rate 策略\n",
    "\n",
    "将每轮的loss,acc存入 rank目录下的record.csv中\n",
    "将每轮最好的结果存入rank目录下的rank.csv中\n",
    "不断更新成绩权重为best.pdparams\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import paddle\n",
    "from paddle.nn import Linear\n",
    "import paddle.nn.functional as F\n",
    "import paddle.nn as nn\n",
    "from visualdl import LogWriter\n",
    "import numpy as np\n",
    "from dataset import Dataset\n",
    "from config import Config\n",
    "import os\n",
    "import pandas as pd\n",
    "import paddlenlp\n",
    "from paddlenlp.transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import functools\n",
    "from paddle.io import DataLoader, BatchSampler\n",
    "from paddlenlp.data import DataCollatorWithPadding\n",
    "from metric import MultiLabelReport\n",
    "from eval import evaluate\n",
    "import time\n",
    "\n",
    "\n",
    "def preprocess_function(examples, tokenizer, max_seq_length):\n",
    "    result = tokenizer(text=examples[\"text\"], max_seq_len=max_seq_length)\n",
    "    result[\"labels\"] = examples[\"labels\"]\n",
    "    return result\n",
    "\n",
    "class Train(object):\n",
    "    batch_size = 30\n",
    "    epochs = 100\n",
    "    data = None\n",
    "    def __init__(self):\n",
    "        cf = Config()\n",
    "        self.cf = cf\n",
    "        self.modelPath = cf.modelPath\n",
    "        self.logPath = cf.logPath\n",
    "        self.pointsPath = cf.pointsPath\n",
    "        self.use_gpu = cf.use_gpu\n",
    "        self.rankPath = cf.rankPath\n",
    "        self.dataset = Dataset()\n",
    "        self.model_name = \"ernie-2.0-base-en\"\n",
    "        self.num_classes = 6 # 分类\n",
    "        self.max_seq_length = 512\n",
    "\n",
    "        if not os.path.exists(self.rankPath):\n",
    "            os.makedirs(self.rankPath)\n",
    "\n",
    "    def run(self):\n",
    "        #开启GPU\n",
    "        paddle.set_device('gpu:0') if self.use_gpu else paddle.set_device('cpu')\n",
    "\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(self.model_name, num_classes=self.num_classes)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "\n",
    "        train_ds = self.dataset.getLoader(mode='train')\n",
    "        valid_ds = self.dataset.getLoader(mode='valid')\n",
    "\n",
    "        trans_func = functools.partial(preprocess_function, tokenizer=tokenizer, max_seq_length=self.max_seq_length)\n",
    "        train_ds = train_ds.map(trans_func)\n",
    "        valid_ds = valid_ds.map(trans_func)\n",
    "\n",
    "        # collate_fn函数构造，将不同长度序列充到批中数据的最大长度，再将数据堆叠\n",
    "        collate_fn = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "\n",
    "        # 定义BatchSampler，选择批大小和是否随机乱序，进行DataLoader\n",
    "        train_batch_sampler = BatchSampler(train_ds, batch_size=self.batch_size, shuffle=True)\n",
    "        valid_batch_sampler = BatchSampler(valid_ds, batch_size=self.batch_size, shuffle=False)\n",
    "        train_data_loader = DataLoader(dataset=train_ds, batch_sampler=train_batch_sampler, collate_fn=collate_fn)\n",
    "        valid_data_loader = DataLoader(dataset=valid_ds, batch_sampler=valid_batch_sampler, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "        self.log_writer = LogWriter(self.cf.logPath)\n",
    "\n",
    "        learning_rate = 3e-5 \n",
    "\n",
    "        # 学习率预热比例\n",
    "        warmup_proportion = 0.1\n",
    "\n",
    "        # 权重衰减系数，类似模型正则项策略，避免模型过拟合\n",
    "        weight_decay = 0.01\n",
    "\n",
    "        num_training_steps = len(train_data_loader) * self.epochs\n",
    "\n",
    "        # 学习率衰减策略\n",
    "        lr_scheduler = paddlenlp.transformers.LinearDecayWithWarmup(learning_rate, num_training_steps, warmup_proportion)\n",
    "\n",
    "        decay_params = [\n",
    "            p.name for n, p in model.named_parameters()\n",
    "            if not any(nd in n for nd in [\"bias\", \"norm\"])\n",
    "        ]\n",
    "        optimizer = paddle.optimizer.AdamW(\n",
    "            learning_rate=lr_scheduler,\n",
    "            parameters=model.parameters(),\n",
    "            weight_decay=weight_decay,\n",
    "            apply_decay_param_fun=lambda x: x in decay_params)\n",
    "\n",
    "        # learning_rate = 1e-4 \n",
    "\n",
    "        # optimizer = paddle.optimizer.Adam(learning_rate=learning_rate, weight_decay=paddle.regularizer.L2Decay(coeff=1e-5), parameters=model.parameters())\n",
    "\n",
    "        iter_ = 0 #迭代次数\n",
    "        for epoch in range(1, self.epochs + 1):\n",
    "            for step, batch in enumerate(train_data_loader, start=1):\n",
    "                input_ids, token_type_ids, labels = batch['input_ids'], batch['token_type_ids'], batch['labels']\n",
    "\n",
    "                # 计算模型输出、损失函数值、分类概率值、准确率、f1分数\n",
    "                logits = model(input_ids, token_type_ids)\n",
    "                loss = F.cross_entropy(logits, labels)\n",
    "                avg_loss = paddle.mean(loss) \n",
    "                acc = paddle.metric.accuracy(input=logits, label=labels)\n",
    "                loss_t = avg_loss.numpy()\n",
    "                acc_t = acc.numpy()\n",
    "                loss_t = np.round(loss_t,2)\n",
    "                acc_t = np.round(acc_t,2)\n",
    "\n",
    "                if step % 10 == 0:\n",
    "                    print(\"Train epoch: {}, batch: {}, loss is: {}, acc is {}\".format(epoch, step, loss_t, acc_t))\n",
    "                    self.log_writer.add_scalar(tag = 'acc', step = iter_, value = acc_t)\n",
    "                    self.log_writer.add_scalar(tag = 'loss', step = iter_, value = loss_t)\n",
    "                \n",
    "                # 反向梯度回传，更新参数\n",
    "                loss.backward()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.step()\n",
    "                optimizer.clear_grad()\n",
    "\n",
    "            loss_e, acc_e = evaluate(model,valid_data_loader)\n",
    "            loss_e = np.round(loss_e,2)\n",
    "            acc_e = np.round(acc_e,2)\n",
    "            print(\"Eval epoch: {}, batch: {}, loss is: {}, acc is {}\".format(epoch, step, loss_e, acc_e))\n",
    "\n",
    "            paddle.save(model.state_dict(), './{}/epoch{}'.format(self.pointsPath,epoch)+'.pdparams')\n",
    "            self.rank(model,epoch,loss_t,loss_e,acc_t,acc_e)\n",
    "            iter_ += 1\n",
    "\n",
    "\n",
    "    def rank(self, model, epoch, loss_t, loss_e, acc_t, acc_e):\n",
    "        if os.path.exists(self.rankPath+'rank.csv') is False:\n",
    "            df = pd.DataFrame([{\n",
    "                \"epoch\":epoch,\n",
    "                \"loss_t\":loss_t, \n",
    "                'loss_e':loss_e,\n",
    "                'acc_t':acc_t,\n",
    "                'acc_e':acc_e,\n",
    "                }])\n",
    "            df.to_csv(self.rankPath+'rank.csv', index=False)\n",
    "            paddle.save(model.state_dict(), self.rankPath+'best.pdparams')\n",
    "            paddle.save(model.state_dict(), self.rankPath+'/records/epoch'+str(epoch)+'.pdparams')\n",
    "        else:\n",
    "            df = pd.read_csv(self.rankPath+'/rank.csv')\n",
    "            maxAccE = df.loc[df.index.max()]['acc_e']\n",
    "            maxAccE = float(maxAccE)\n",
    "            if acc_e > maxAccE:\n",
    "                df.loc[df.index.max() + 1] = [epoch, loss_t, loss_e, acc_t, acc_e]\n",
    "                df.to_csv(self.rankPath+'rank.csv', index=False)\n",
    "                paddle.save(model.state_dict(), self.rankPath+'best.pdparams')\n",
    "                paddle.save(model.state_dict(), self.rankPath+'/records/epoch'+str(epoch)+'.pdparams')\n",
    "\n",
    "        if os.path.exists(self.rankPath+'record.csv') is False:\n",
    "            init = {\n",
    "                \"epoch\":epoch,\n",
    "                \"loss_t\":loss_t, \n",
    "                'loss_e':loss_e,\n",
    "                'acc_t':acc_t,\n",
    "                'acc_e':acc_e,\n",
    "            }\n",
    "            dfr = pd.DataFrame([init])\n",
    "            dfr.to_csv(self.rankPath+'record.csv', index=False)\n",
    "        else:\n",
    "            dfr = pd.read_csv(self.rankPath+'record.csv')\n",
    "            dfr.loc[dfr.index.max() + 1] = [\n",
    "                epoch, \n",
    "                loss_t, \n",
    "                loss_e, \n",
    "                acc_t, \n",
    "                acc_e\n",
    "            ]\n",
    "            dfr.to_csv(self.rankPath+'record.csv', index=False)\n",
    "\n",
    "        \n",
    "train = Train()\n",
    "train.run()\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 四.生成比赛结果文件\n",
    "\n",
    "通过读取rank目录下权重，进行结果预测\n",
    "\n",
    "```\n",
    "model.set_dict(paddle.load(self.rankPath+'/best.pdparams'))\n",
    "#model.set_dict(paddle.load(self.pointsPath+'/epoch40.pdparams'))\n",
    "```\n",
    "\n",
    "结果保存在rank目录下\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#predict.py\n",
    "import paddle\n",
    "from dataset import Dataset\n",
    "from config import Config\n",
    "import os\n",
    "import pandas as pd\n",
    "from paddlenlp.transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import functools\n",
    "from paddle.io import DataLoader, BatchSampler\n",
    "from paddlenlp.data import DataCollatorWithPadding\n",
    "import copy\n",
    "\n",
    "\n",
    "def preprocess_function(examples, tokenizer, max_seq_length):\n",
    "    result = tokenizer(text=examples[\"text\"], max_seq_len=max_seq_length)\n",
    "    result[\"labels\"] = examples[\"labels\"]\n",
    "    #result[\"id\"] = examples[\"id\"]\n",
    "    return result\n",
    "\n",
    "class Predict(object):\n",
    "    def __init__(self):\n",
    "        cf = Config()\n",
    "        self.cf = cf\n",
    "        self.logPath = cf.logPath\n",
    "        self.pointsPath = cf.pointsPath\n",
    "        self.use_gpu = cf.use_gpu\n",
    "        self.rankPath = cf.rankPath\n",
    "        self.dataset = Dataset()\n",
    "        self.model_name = \"ernie-2.0-base-en\"\n",
    "        self.num_classes = 6 # 分类\n",
    "        self.max_seq_length = 512\n",
    "        self.batch_size = 1\n",
    "\n",
    "        if not os.path.exists(self.rankPath):\n",
    "            os.makedirs(self.rankPath)\n",
    "\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        #开启GPU\n",
    "        paddle.set_device('gpu:0') if self.use_gpu else paddle.set_device('cpu')\n",
    "\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(self.model_name, num_classes=self.num_classes)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "\n",
    "        test_src = self.dataset.getLoader(mode='test')\n",
    "        test_ds = copy.deepcopy(test_src)\n",
    "        \n",
    "        trans_func = functools.partial(preprocess_function, tokenizer=tokenizer, max_seq_length=self.max_seq_length)\n",
    "        test_ds = test_ds.map(trans_func)\n",
    "\n",
    "        # collate_fn函数构造，将不同长度序列充到批中数据的最大长度，再将数据堆叠\n",
    "        collate_fn = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "\n",
    "        # 定义BatchSampler，选择批大小和是否随机乱序，进行DataLoader\n",
    "        test_batch_sampler = BatchSampler(test_ds, batch_size=self.batch_size, shuffle=False)\n",
    "        test_data_loader = DataLoader(dataset=test_ds, batch_sampler=test_batch_sampler, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "        model.set_dict(paddle.load(self.rankPath+'/best.pdparams'))\n",
    "        #model.set_dict(paddle.load(self.pointsPath+'/epoch40.pdparams'))\n",
    "        model.eval()\n",
    "        results = []\n",
    "\n",
    "        index = 0\n",
    "        for batch in test_data_loader:\n",
    "            \n",
    "            input_ids, token_type_ids = batch['input_ids'], batch['token_type_ids']\n",
    "            logits = model(input_ids, token_type_ids)\n",
    "            label = paddle.argmax(logits).numpy()[0]\n",
    "            results.append([test_src[index]['id'], label])\n",
    "            print(test_src[index]['id'], label)\n",
    "            index += 1\n",
    "        submit = pd.DataFrame(results, columns=['id', 'predict'])\n",
    "        submit[['id', 'predict']].to_csv(self.rankPath + '/submit_example.csv', index=False)\n",
    "        \n",
    "predict = Predict()\n",
    "predict.run()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 五.其他配置\n",
    "\n",
    "日志路径，数据集路径，排名路径可以config.py中进行设置\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "class Config(object):\n",
    "    def __init__(self):\n",
    "        self.dataPath = '../data/data168015/data' # 原始数据集路径\n",
    "        self.minePath = './data/' # 生成的数据集路径\n",
    "        self.trainRatio = '0.8' # 训练集和验证集比例\n",
    "        self.modelPath = './model/'\n",
    "        self.rankPath = './rank/'\n",
    "        self.logPath = './logs' \n",
    "        self.pointsPath = './checkpoint' # 每轮保存的权重\n",
    "        self.inferencePath = './inference/'\n",
    "        self.use_gpu = 1 # 是否使用GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 六.总结\n",
    "\n",
    "目前得分在0.949左右，还有很大提升空间\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/6955a9da1d7c4494b05ae492de83cdeccca72f345b214468bb1a3885b0a4bed7)\n",
    "\n",
    "1.数据集方向，通过调整jieba提取关键字的参数，获取不同的关键字\n",
    "```\n",
    "r0 = jieba.analyse.extract_tags(df.loc[j, 'user_agent'], topK=10)\n",
    "item.extend(r0)\n",
    "r1 = jieba.analyse.extract_tags(df.loc[j, 'url'], topK=20)\n",
    "item.extend(r1)\n",
    "r2 = jieba.analyse.extract_tags(df.loc[j, 'refer'], topK=10)\n",
    "item.extend(r2)\n",
    "r3 = jieba.analyse.extract_tags(df.loc[j, 'body'], topK=20)\n",
    "```\n",
    "\n",
    "2.通过修改分配训练集的比例增大训练集范围\n",
    "```\n",
    "self.trainRatio = '0.8'\n",
    "```\n",
    "3.训练方向，微调batch_size,learning_rate,weight_decay等\n",
    "\n",
    "4.模型方向，尝试替换不同的模型，调整max_seq_length数值等\n",
    "\n",
    "\n",
    "\n",
    "相关比赛索引：\n",
    "\n",
    "[2022 CCF BDCI 大赛之 高端装备制造知识图谱自动化构建技术评测任务](https://aistudio.baidu.com/aistudio/projectdetail/4485704)\n",
    "\n",
    "[2022 CCF BDCI 大赛之返乡发展人群预测](https://aistudio.baidu.com/aistudio/projectdetail/4477656)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
